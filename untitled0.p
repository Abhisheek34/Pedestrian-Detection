# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10voHfjfkhKlPW0UhbcA_Fd46anQzGbNv
"""

import numpy as np
import matplotlib.pyplot as plt
from IPython.display import Image, display

!pip install ultralytics==8.1.25
!pip install opencv-python

# Phase 2  : load YOLOv8 model (pretrained on COCO)

"""# Phase 2  : load YOLOv8 model (pretrained on COCO)"""

import torch
print(torch.__version__)  # Should show 2.1.x to 2.3.x ideally (not 2.6 for now)

# Downgrade PyTorch to 2.3.0 with CUDA 11.8 (stable with YOLOv8)
!pip install torch==2.3.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Reinstall Ultralytics (YOLOv8)
#!pip install --upgrade --no-cache-dir ultralytics

from ultralytics import YOLO

model = YOLO('yolov8n.pt')

print(model.names)



!wget https://github.com/ultralytics/yolov5/releases/download/v1.0/zidane.mp4 -O driving_sample.mp4

from google.colab import files

uploaded = files.upload()  # Choose your video file (MP4 preferred)
video_path = next(iter(uploaded))
print("Uploaded video:", video_path)



"""# Phase 3: Upload a Video for Simulated Real-Time Detection"""

from google.colab import files

# Upload your dashcam or street-view video here (MP4 preferred)
uploaded = files.upload()

video_path = next(iter(uploaded))  # Gets the filename
print(f"Uploaded video: {video_path}")

import cv2

cap = cv2.VideoCapture(video_path)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Resize frame for faster processing
    frame = cv2.resize(frame, (640, 360))

    # Run YOLO detection
    results = model(frame)[0]

    # Draw bounding boxes for 'person' class only (class 0)
    for box in results.boxes:
        cls = int(box.cls[0])
        conf = float(box.conf[0])
        if cls == 0 and conf > 0.4:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, f"Person {conf:.2f}", (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

    # Display the frame (resize for Colab visualization)
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    plt.imshow(frame_rgb)
    plt.axis('off')
    plt.show()

    # Simulate real-time by breaking after a few frames (Colab can't stream)
    break  # Remove this for full video

cap.release()

# Run full detection and save result video
cap = cv2.VideoCapture(video_path)
output = cv2.VideoWriter('output.avi', cv2.VideoWriter_fourcc(*'XVID'), 20, (640, 360))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame = cv2.resize(frame, (640, 360))
    results = model(frame)[0]

    for box in results.boxes:
        cls = int(box.cls[0])
        conf = float(box.conf[0])
        if cls == 0 and conf > 0.4:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, f"Person {conf:.2f}", (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

    output.write(frame)

cap.release()
output.release()
print("Detection video saved as output.avi")

from google.colab import files
files.download('output.avi')













